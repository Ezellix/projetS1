<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="En 2019 est sortie dans la revue du MIT « TechnologyReview » un article abordant la relation problématique entre intelligence artificielle et pollution...">
    <meta name="keywords" content="MIT, TechnologyReview, AI, carbon, environment">
    <link rel="stylesheet" href="css/lib/normalize.css">
    <link rel="stylesheet" href="css/mescss/typography.css">
    <link rel="stylesheet" href="css/mescss/menu.css">
    <link rel="stylesheet" href="css/mescss/layout.css">
    <link rel="icon" href="/img/logo.png">
    <title>Article - TechnologyReview</title>
</head>
<body>
<header>
    <a href="index.html"><img src="img/logo.png" alt="Le logo du site - IA" height="50" width="50"></a>
    <p>IA - Un avenir pas si rose ?</p>
    <div class="nav-toggle" onclick='document.documentElement.classList.toggle("menu-open")'>
        <div class="nav-toggle-bar"></div>
    </div>
    <nav class="menu">
        <ul>
            <li class="menu-lien-principal"><a href="index.html">Accueil</a></li>
            <li class="menu-lien-principal"><a href="her.html">Her</a></li>
            <li class="menu-lien-principal"><a href="pollution.html">La pollution numérique</a></li>
            <li class="menu-lien-principal"><a href="solution.html">Que faire ?</a></li>
            <li class="menu-lien-secondaire"><a href="">Article du MIT</a></li>
            <li class="menu-lien-secondaire"><a href="autres-projets.html">Autres projets</a></li>
            <li class="menu-lien-secondaire"><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
</header>
<main class="annexe article-scientifique">
    <article>
    <h1>"Training a single AI model can emit as much carbon as five cars in their lifetimes"</h1>
    <p>Publié le 06/06/2019,</p>
    <p>par Karen Hao</p>

    <p>The artificial-intelligence industry is often compared to the oil industry: once mined and refined, data, like oil, can be a highly lucrative commodity. Now it seems the metaphor may extend even further. Like its fossil-fuel counterpart, the process of deep learning has an outsize <strong>environmental</strong> impact.</p>
    <p>In a new paper, <strong>researchers</strong> at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large <strong>AI</strong> models. They found that the process can emit more than 626,000 pounds of <strong>carbon</strong> dioxide equivalent—nearly five times the lifetime emissions of the average American car (and that includes manufacture of the car itself).</p>
    <p>It’s a jarring quantification of something <strong>AI</strong> <strong>researchers</strong> have suspected for a long time. “While probably many of us have thought of this in an abstract, vague level, the figures really show the magnitude of the problem,” says Carlos Gómez-Rodríguez, a computer scientist at the University of A Coruña in Spain, who was not involved in the <strong>research</strong>. “Neither I nor other <strong>researchers</strong> I’ve discussed them with thought the <strong>environmental</strong> impact was that substantial.”</p>

    <h2>The carbon footprint of natural-language processing</h2>
    <p>The paper specifically examines the model training process for natural-language processing (NLP), the subfield of <strong>AI</strong> that focuses on teaching machines to handle human language. In the last two years, the NLP community has reached several noteworthy performance milestones in machine translation, sentence completion, and other standard benchmarking tasks. OpenAI’s infamous GPT-2 model, as one example, excelled at writing convincing fake news articles.</p>
    <p>But such advances have required training ever larger models on sprawling data sets of sentences scraped from the internet. The approach is computationally expensive—and highly energy intensive.</p>
    <p>The <strong>researchers</strong> looked at four models in the field that have been responsible for the biggest leaps in performance: the Transformer, ELMo, BERT, and GPT-2. They trained each on a single GPU for up to a day to measure its power draw. They then used the number of training hours listed in the model’s original papers to calculate the total energy consumed over the complete training process. That number was converted into pounds of <strong>carbon</strong> dioxide equivalent based on the average energy mix in the US, which closely matches the energy mix used by Amazon’s AWS, the largest cloud services provider.</p>
    <p>They found that the computational and <strong>environmental</strong> costs of training grew proportionally to model size and then exploded when additional tuning steps were used to increase the model’s final accuracy. In particular, they found that a tuning process known as neural architecture search, which tries to optimize a model by incrementally tweaking a neural network’s design through exhaustive trial and error, had extraordinarily high associated costs for little performance benefit. Without it, the most costly model, BERT, had a <strong>carbon</strong> footprint of roughly 1,400 pounds of <strong>carbon</strong> dioxide equivalent, close to a round-trip trans-America flight for one person.</p>
    <p>What’s more, the <strong>researchers</strong> note that the figures should only be considered as baselines. “Training a single model is the minimum amount of work you can do,” says Emma Strubell, a PhD candidate at the University of Massachusetts, Amherst, and the lead author of the paper. In practice, it’s much more likely that <strong>AI</strong> <strong>researchers</strong> would develop a new model from scratch or adapt an existing model to a new data set, either of which can require many more rounds of training and tuning.</p>
    <p>To get a better handle on what the full development pipeline might look like in terms of <strong>carbon</strong> footprint, Strubell and her colleagues used a model they’d produced in a previous paper as a case study. They found that the process of building and testing a final paper-worthy model required training 4,789 models over a six-month period. Converted to CO2 equivalent, it emitted more than 78,000 pounds and is likely representative of typical work in the field.</p>
    <p>“What probably many of us did not comprehend is the scale of it until we saw these comparisons,” echoed Siva Reddy, a postdoc at Stanford University who was not involved in the <strong>research</strong>.</p>
    <p>The significance of those figures is colossal—especially when considering the current trends in <strong>AI</strong> <strong>research</strong>. “In general, much of the latest <strong>research</strong> in <strong>AI</strong> neglects efficiency, as very large neural networks have been found to be useful for a variety of tasks, and companies and institutions that have abundant access to computational resources can leverage this to obtain a competitive advantage,” Gómez-Rodríguez says. “This kind of analysis needed to be done to raise awareness about the resources being spent [...] and will spark a debate.”</p>
    </article>
</main>
<aside class="article-suivant">
    <a href="solution.html"><p>Que pouvons-nous faire ?</p></a>
</aside>
<footer>
    <p>Charles Mangin, étudiant MMi Montbéliard (B2)</p>
    <a href="mailto:charlesmangin@edu.univ-fcomte.fr">charlesmangin@edu.univ-fcomte.fr</a>
    <p>Projet réalisé dans le cadre d’un exercice pédagogique au <a href="https://mmimontbeliard.com/">département MMi de Montbéliard</a>.</p>
    <p>Tous droits de reproduction et de diffusion réservés © 2020</p>
</footer>
</body>
</html>